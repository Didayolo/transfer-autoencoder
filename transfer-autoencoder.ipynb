{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer AE\n",
    "\n",
    "_Use an autoencoder pre-trained on many sources as tranfer-learning preprocessing (feature extractor)._\n",
    "\n",
    "## What?\n",
    "\n",
    "![transferAE](transferAE.jpg)\n",
    "\n",
    "## Why?\n",
    "\n",
    "* Toute donnée est bonne à prendre (data driven). Les datasets s'entraident malgré la différence de leurs tâches. C'est la similitude entre les P(X) qui nous intéresse en premier lieu et aide par la suite à calculer les P(y|X) respectifs. Ainsi on peut tirer un avantage de données de sources très variées (different shapes and tasks).\n",
    "\n",
    "* **Hypothèse : Il y a des connaissances générales communes à des données d'apparence éloignées.** L'espace des possibles est extrêmement grand. La \"fenêtre\" (sous-espace) ouverte par les données est très précises au final donc toute donnée affine un modèle (e.g. autoencoder). Autrement dit, l'ensemble des entrées plausibles est très petit.\n",
    "\n",
    "* En gros on compresse des données pour les emmener avec soit.\n",
    "\n",
    "**Plasticité : taux d'apprentissage de l'encoder (notamment pendant l'apprentissage d'un modèle).**\n",
    "\n",
    "Pas forcément besoin d'une plasticité élevée pour bien généraliser?\n",
    "\n",
    "\n",
    "L'AE apprend quelque chose de beaucoup plus général qu'un discriminant sur les données car il doit reconstituer l'information et non pas simplement répondre à une question dessus. **C'est une bonne chose pour généraliser mais il s'agit d'un problème beaucoup plus difficile, l'entrainement pourrait donc s'avérer moins efficace.**\n",
    "\n",
    "\n",
    "## How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Input shaper\n",
    "\n",
    "On souhaite que le modèle puisse gérer des input data de differentes shapes.\n",
    "\n",
    "* Soit un réseau capable de gerer des entrées de taille différentes;\n",
    "* Soit un processing qui les mets au même format (padding and/or resizing)\n",
    "\n",
    "\n",
    "#### One shape to rule them all\n",
    "\n",
    "4D tensor : (row_count, col_count, channel, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : put imports and loading functions in utilities\n",
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "Model = tf.keras.models.Model\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Input, Dense, Conv2D, Conv3D, MaxPooling2D, MaxPooling3D, UpSampling2D, UpSampling3D, Flatten, Reshape, Dropout = tf.keras.layers.Input, tf.keras.layers.Dense, tf.keras.layers.Conv2D, tf.keras.layers.Conv3D, tf.keras.layers.MaxPooling2D, tf.keras.layers.MaxPooling3D, tf.keras.layers.UpSampling2D, tf.keras.layers.UpSampling3D, tf.keras.layers.Flatten, tf.keras.layers.Reshape, tf.keras.layers.Dropout\n",
    "regularizers = tf.keras.regularizers\n",
    "load_model = tf.keras.models.load_model\n",
    "from utilities import *\n",
    "sys.path.append('/home/adrien/Documents/autodl/codalab_competition_bundle/AutoDL_starting_kit/AutoDL_ingestion_program/')\n",
    "from dataset import AutoDLDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "models_path = 'models'\n",
    "data_path = 'data'\n",
    "load = True\n",
    "save = True\n",
    "verbose = False\n",
    "autoencoder_name = 'autoencoder'\n",
    "autoencoder_path = os.path.join(models_path, autoencoder_name+'.h5')\n",
    "model_name = 'model'\n",
    "model_path = os.path.join(models_path, model_name+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "dataset_name = 'munster'\n",
    "input_dir = os.path.join(data_path, dataset_name)\n",
    "basename = dataset_name + '.data' # why?\n",
    "D_train = AutoDLDataset(os.path.join(input_dir, basename, 'train'))\n",
    "D_test = AutoDLDataset(os.path.join(input_dir, basename, 'test'))\n",
    "x_train, y_train = input_fn(D_train.get_dataset())\n",
    "x_test, y_test = input_fn(D_test.get_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fea36ec4400>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADDNJREFUeJzt3X+s3fVdx/Hn23op2nWxda42rJGNdCgsDtxNt2xotuEQmkVYTJBqlppguj8gbslMJNNE/iTqWGZClnRS6cxkUxmhJsSNVSNZIoQL6QqUUZAV11pal+JgJpZL+/aP++28g3u+93LO95zvad/PR3Jyvufz+Z7zfefbvvr98Tk9n8hMJNXzE30XIKkfhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlE/OcmNnRer83zWTHKTUin/y//wSp6Mlaw7Uvgj4mrg88Aq4K8y87a29c9nDe+NK0fZpKQWD+feFa879Gl/RKwC7gCuAS4BtkXEJcN+nqTJGuWafwvwbGY+l5mvAF8Bru2mLEnjNkr4LwC+t+j14abtx0TEjoiYi4i5eU6OsDlJXRr73f7M3JmZs5k5O8PqcW9O0gqNEv4jwKZFr9/WtEk6C4wS/keAzRHx9og4D7gB2NNNWZLGbeihvsx8NSJuBr7OwlDfrsx8srPKJI3VSOP8mXk/cH9HtUiaIL/eKxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRE52iW5N38K/f09r/3d+4s7X/9hPvaO3/5vWzrf2nDhxs7Vd/PPJLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlEjjfNHxCHgZeAU8Gpmtg/6aixWXXrxwL77PnRH63vnc6a1/6Z1T7f2/8MvX9Xav/ZAa7d61MWXfD6Umd/v4HMkTZCn/VJRo4Y/gW9ExKMRsaOLgiRNxqin/Vdk5pGIeCvwQER8JzMfXLxC84/CDoDz+ekRNyepKyMd+TPzSPN8HLgX2LLEOjszczYzZ2dYPcrmJHVo6PBHxJqIWHtmGbgKeKKrwiSN1yin/RuAeyPizOf8bWb+UydVSRq7ocOfmc8B7+6wFg3ryAsDu/7g4A2tb33g0nu6rkZnCYf6pKIMv1SU4ZeKMvxSUYZfKsrwS0X5093ngFP//YOBfc8f3tz+5ks7LkZnDY/8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU4/zngFUb3jqw71d/ySmytTSP/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlOP854K1awZ2bV3/yFg3ffw90dr/M/vfObDv1AG/g9Anj/xSUYZfKsrwS0UZfqkowy8VZfilogy/VNSy4/wRsQv4KHA8M9/VtK0HvgpcCBwCrs/MF8dXptqceva7A/v+5B9/u/W9v7XtjpG2/eTv/GVr/+U/+OTAvk2O8/dqJUf+u4CrX9N2C7A3MzcDe5vXks4iy4Y/Mx8ETrym+Vpgd7O8G7iu47okjdmw1/wbMvNos/wCsKGjeiRNyMg3/DIzgRzUHxE7ImIuIubmOTnq5iR1ZNjwH4uIjQDN8/FBK2bmzsyczczZGVYPuTlJXRs2/HuA7c3yduC+bsqRNCnLhj8i7gb+Dbg4Ig5HxI3AbcBHIuIZ4Neb15LOIsuO82fmtgFdV3Zci8bgoj98qH2FQX+6Ouf5DT+pKMMvFWX4paIMv1SU4ZeKMvxSUf50d3Ezsaq1f37gF7d1tvPILxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFOc5f3Hyeau0/zekJVaJJ88gvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRhl8qyvBLRS0b/ojYFRHHI+KJRW23RsSRiNjXPLaOt0xJXVvJkf8u4Ool2j+XmZc1j/u7LUvSuC0b/sx8EDgxgVokTdAo1/w3R8T+5rJgXWcVSZqIYcP/BeAi4DLgKPDZQStGxI6ImIuIuXlODrk5SV0bKvyZeSwzT2XmaeCLwJaWdXdm5mxmzs6wetg6JXVsqPBHxMZFLz8GPDFoXUnTadmf7o6Iu4EPAm+JiMPAnwIfjIjLgAQOAZ8YY42SxmDZ8GfmtiWa7xxDLerBTKxq7Z/P0T7/ze8/PtoHaGz8hp9UlOGXijL8UlGGXyrK8EtFGX6pKKfoLm7cU3T/67vvHtj3m++7sf3ND+0fadtq55FfKsrwS0UZfqkowy8VZfilogy/VJThl4pynL+4X/zn32/tP/DhnWPb9sEd57X2v/OhsW1aeOSXyjL8UlGGXyrK8EtFGX6pKMMvFWX4paIc5y9u9cGfal/hw5OpQ5PnkV8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXiorM9jmYI2IT8CVgA5DAzsz8fESsB74KXAgcAq7PzBfbPuvNsT7fG1d2ULYmZdt3/rO1/3fXHh36s5ebHvyaa5aaHf7/nf72U0Nv+1z1cO7lpTwRK1l3JUf+V4FPZ+YlwPuAmyLiEuAWYG9mbgb2Nq8lnSWWDX9mHs3Mx5rll4GngAuAa4HdzWq7gevGVaSk7r2ha/6IuBC4HHgY2JCZZ875XmDhskDSWWLF4Y+INwH3AJ/KzJcW9+XCjYMlbx5ExI6ImIuIuXlOjlSspO6sKPwRMcNC8L+cmV9rmo9FxMamfyNwfKn3ZubOzJzNzNkZVndRs6QOLBv+iAjgTuCpzLx9UdceYHuzvB24r/vyJI3LSv5L7weAjwOPR8S+pu0zwG3A30XEjcDzwPXjKVF9uus/3t/av+3Svx/6s+fbR5k1ZsuGPzO/BQwaN3TQXjpL+Q0/qSjDLxVl+KWiDL9UlOGXijL8UlH+dLdanbzr59tX+PPJ1KHueeSXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIc51erdftOtPbf8eLFrf03rXu6y3LUIY/8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1TUslN0d8kpuqXx6nqKbknnIMMvFWX4paIMv1SU4ZeKMvxSUYZfKmrZ8EfEpoj4l4g4EBFPRsQnm/ZbI+JIROxrHlvHX66krqzkxzxeBT6dmY9FxFrg0Yh4oOn7XGb+xfjKkzQuy4Y/M48CR5vllyPiKeCCcRcmabze0DV/RFwIXA483DTdHBH7I2JXRKwb8J4dETEXEXPznBypWEndWXH4I+JNwD3ApzLzJeALwEXAZSycGXx2qfdl5s7MnM3M2RlWd1CypC6sKPwRMcNC8L+cmV8DyMxjmXkqM08DXwS2jK9MSV1byd3+AO4EnsrM2xe1b1y02seAJ7ovT9K4rORu/weAjwOPR8S+pu0zwLaIuAxI4BDwibFUKGksVnK3/1vAUv8/+P7uy5E0KX7DTyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VNREp+iOiP8Cnl/U9Bbg+xMr4I2Z1tqmtS6wtmF1WdsvZObPrWTFiYb/dRuPmMvM2d4KaDGttU1rXWBtw+qrNk/7paIMv1RU3+Hf2fP220xrbdNaF1jbsHqprddrfkn96fvIL6knvYQ/Iq6OiKcj4tmIuKWPGgaJiEMR8Xgz8/Bcz7XsiojjEfHEorb1EfFARDzTPC85TVpPtU3FzM0tM0v3uu+mbcbriZ/2R8Qq4CDwEeAw8AiwLTMPTLSQASLiEDCbmb2PCUfErwE/BL6Ume9q2v4MOJGZtzX/cK7LzD+aktpuBX7Y98zNzYQyGxfPLA1cB/wePe67lrqup4f91seRfwvwbGY+l5mvAF8Bru2hjqmXmQ8CJ17TfC2wu1nezcJfnokbUNtUyMyjmflYs/wycGZm6V73XUtdvegj/BcA31v0+jDTNeV3At+IiEcjYkffxSxhQzNtOsALwIY+i1nCsjM3T9JrZpaemn03zIzXXfOG3+tdkZm/AlwD3NSc3k6lXLhmm6bhmhXN3DwpS8ws/SN97rthZ7zuWh/hPwJsWvT6bU3bVMjMI83zceBepm/24WNnJkltno/3XM+PTNPMzUvNLM0U7LtpmvG6j/A/AmyOiLdHxHnADcCeHup4nYhY09yIISLWAFcxfbMP7wG2N8vbgft6rOXHTMvMzYNmlqbnfTd1M15n5sQfwFYW7vj/O/DHfdQwoK53AN9uHk/2XRtwNwungfMs3Bu5EfhZYC/wDPBNYP0U1fY3wOPAfhaCtrGn2q5g4ZR+P7CveWzte9+11NXLfvMbflJR3vCTijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1TU/wHaF8sVZg/TrwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests, visualization\n",
    "with tf.Session() as sess:\n",
    "    for _ in range(4):\n",
    "        tensor_4d, labels = sess.run(x_train), sess.run(y_train)\n",
    "        \n",
    "input_shape = tensor_4d.shape[1:]\n",
    "print(input_shape)\n",
    "print(labels)\n",
    "plt.imshow(tensor_4d.reshape((28,28))) #((32,32,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Autoencoder\n",
    "\n",
    "Unsupervised, no task. Train on every datasets.\n",
    "\n",
    "Plutôt que d'avoir un modèle génératif qui permet de faire de l'augmentation de données, celui-ci apprend une représentation abstraite des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder initialized.\n"
     ]
    }
   ],
   "source": [
    "# Load or initialize autoencder\n",
    "if os.path.isfile(autoencoder_path) and load:\n",
    "    autoencoder = load_model(autoencoder_path)\n",
    "    print('{} loaded.'.format(autoencoder_name))\n",
    "\n",
    "else:\n",
    "    kernel = (1, 3, 3)\n",
    "    pool = (1, 2, 2)\n",
    "    strides = (2,2,2)\n",
    "    autoencoder = Sequential()\n",
    "\n",
    "    # Encoder Layers\n",
    "    autoencoder.add(Conv3D(16, kernel, activation='relu', padding='same', input_shape=input_shape))\n",
    "    autoencoder.add(MaxPooling3D(pool, padding='same'))\n",
    "    autoencoder.add(Conv3D(8, kernel, activation='relu', padding='same'))\n",
    "    autoencoder.add(MaxPooling3D(pool, padding='same'))\n",
    "    autoencoder.add(Conv3D(8, kernel, strides=strides, activation='relu', padding='same'))\n",
    "\n",
    "    # Flatten encoding for visualization\n",
    "    autoencoder.add(Flatten())\n",
    "    autoencoder.add(Reshape((1, 4, 4, 8)))\n",
    "\n",
    "    # Decoder Layers\n",
    "    autoencoder.add(Conv3D(8, kernel, activation='relu', padding='same'))\n",
    "    autoencoder.add(UpSampling3D(pool))\n",
    "    autoencoder.add(Conv3D(8, kernel, activation='relu', padding='same'))\n",
    "    autoencoder.add(UpSampling3D(pool))\n",
    "    autoencoder.add(Conv3D(16, kernel, activation='relu'))\n",
    "    autoencoder.add(UpSampling3D(pool))\n",
    "    autoencoder.add(Conv3D(1, kernel, activation='sigmoid', padding='same'))\n",
    "    print('{} initialized.'.format(autoencoder_name))\n",
    "    \n",
    "    if verbose:\n",
    "        autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('flatten', 5).output)\n",
    "\n",
    "if verbose:\n",
    "    encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 84s 8ms/step - loss: 0.1027 - val_loss: 0.0972\n",
      "autoencoder saved.\n"
     ]
    }
   ],
   "source": [
    "# Train and save autoencoder\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train, epochs=1, steps_per_epoch=10000, validation_data=(x_test, x_test), validation_steps=1000)\n",
    "\n",
    "if save:\n",
    "    autoencoder.save(autoencoder_path)\n",
    "    print('{} saved.'.format(autoencoder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe9a15cf940>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconstructed_tensor_4d = autoencoder.predict(tensor_4d)[0]\n",
    "plt.imshow(reconstructed_tensor_4d.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model\n",
    "\n",
    "Un discriminant unique pour chaque problème de chaque dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized.\n"
     ]
    }
   ],
   "source": [
    "# Load or define model\n",
    "# TODO : decide if it loads the encoder part\n",
    "if os.path.isfile(model_path) and load:\n",
    "    model = load_model(model_path)\n",
    "    print('{} loaded.'.format(model_name))\n",
    "\n",
    "else:\n",
    "    model = Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(Dense(512, activation=tf.nn.relu))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation=tf.nn.sigmoid))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print('{} initialized.'.format(model_name))\n",
    "    \n",
    "if verbose:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 66s 7ms/step - loss: 0.1459 - acc: 0.9576\n",
      "5000/5000 [==============================] - 7s 1ms/step\n",
      "[0.0, 0.0956]\n",
      "model saved.\n"
     ]
    }
   ],
   "source": [
    "# Train, save and evaluate model\n",
    "model.fit(x_train, y_train, epochs=1, steps_per_epoch=10000)\n",
    "print(model.evaluate(x_test, y_test, steps=5000))\n",
    "\n",
    "if save:\n",
    "    model.save(model_path)\n",
    "    print('{} saved.'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model : SVM\n",
    "#from sklearn.svm import SVC\n",
    "\n",
    "#clf = SVC(C=6, kernel='poly')\n",
    "\n",
    "# extract features\n",
    "#features_train = encoder.predict(x_train, steps=1)\n",
    "#features_test = encoder.predict(x_test, steps=1)\n",
    "\n",
    "#clf.fit(features_train, y_train)\n",
    "#clf.score(features_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p35_jupyter",
   "language": "python",
   "name": "p35_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
