{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer AE\n",
    "\n",
    "Use a pre-trained autoencoder as tranfer-learning preprocessing (feature extractor).\n",
    "\n",
    "## What?\n",
    "\n",
    "![transferAE](transferAE.jpg)\n",
    "\n",
    "## Why?\n",
    "\n",
    "Je vous le donne en 5 :\n",
    "\n",
    "* Toute donnée est bonne à prendre (data driven).\n",
    "\n",
    "* Les datasets marchent main dans la main.\n",
    "\n",
    "* L'espace des possibles est extrêmement grand. La fenêtre ouverte par les données est très précises au final donc toute données affine un modèle (e.g. autoencoder). Autrement dit, l'ensemble des signaux plausibles est très petit.\n",
    "\n",
    "* En gros on compresse des données pour les emmener avec soit.\n",
    "\n",
    "* Ainsi on peut tirer un avantage de données de sources très variées (different shapes and tasks).\n",
    "\n",
    "Pas forcément besoin d'une plasticité élevée pour bien généraliser\n",
    "\n",
    "Plasticité : taux d'apprentissage de l'encoder (notamment pendant l'apprentissage d'un modèle)\n",
    "\n",
    "\n",
    "//Test sur AutoDL ?\n",
    "//Est ce que avec un 3D AE train sur tous les datasets ca peut marcher ? (Padding)\n",
    "//Ou il faut être domaine specific ?\n",
    "//Il pourrait peut-être maîtriser plusieurs domaine grâce à une certaine plasticité. On le re train un peu sur le nouveau //dataset pour l'adapter et lui rappeler le bon domaine ?\n",
    "\n",
    "## How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Input shaper\n",
    "\n",
    "Input data of different shapes.\n",
    "\n",
    "* Soit un réseau capable de gerer des entrées de taille différentes;\n",
    "* Soit un processing qui les mets au même format (padding and/or resizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(list(map(lambda x: np.expand_dims(x, axis=2), x_train)))\n",
    "x_test = np.array(list(map(lambda x: np.expand_dims(x, axis=2), x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Autoencoder\n",
    "\n",
    "Unsupervised, no task. Train on every datasets.\n",
    "\n",
    "Plutôt que d'avoir un modèle génératif qui permet de faire de l'augmentation de données, celui-ci apprend une représentation abstraite des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "Model = tf.keras.models.Model\n",
    "Sequential = tf.keras.models.Sequential\n",
    "Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape = tf.keras.layers.Input, tf.keras.layers.Dense, tf.keras.layers.Conv2D, tf.keras.layers.MaxPooling2D, tf.keras.layers.UpSampling2D, tf.keras.layers.Flatten, tf.keras.layers.Reshape\n",
    "regularizers = tf.keras.regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#row_shape, col_shape = x_train.shape[1:]\n",
    "#channel_shape = 1\n",
    "#input_shape = (row_shape, col_shape, channel_shape)\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 4, 4, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 8, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Sequential()\n",
    "\n",
    "# Encoder Layers\n",
    "autoencoder.add(Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "autoencoder.add(MaxPooling2D((2, 2), padding='same'))\n",
    "autoencoder.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "autoencoder.add(MaxPooling2D((2, 2), padding='same'))\n",
    "autoencoder.add(Conv2D(8, (3, 3), strides=(2,2), activation='relu', padding='same'))\n",
    "\n",
    "# Flatten encoding for visualization\n",
    "autoencoder.add(Flatten())\n",
    "autoencoder.add(Reshape((4, 4, 8)))\n",
    "\n",
    "# Decoder Layers\n",
    "autoencoder.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "autoencoder.add(UpSampling2D((2, 2)))\n",
    "autoencoder.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "autoencoder.add(UpSampling2D((2, 2)))\n",
    "autoencoder.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "autoencoder.add(UpSampling2D((2, 2)))\n",
    "autoencoder.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('flatten_1').output)\n",
    "#encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 47s 781us/step - loss: 27.9573 - val_loss: 17.8638\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 43s 721us/step - loss: 16.4896 - val_loss: 15.3050\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 43s 721us/step - loss: 14.7836 - val_loss: 14.1191\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 42s 705us/step - loss: 13.8792 - val_loss: 13.5888\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 42s 703us/step - loss: 13.3217 - val_loss: 12.9245\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 42s 693us/step - loss: 12.9134 - val_loss: 12.5840\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 42s 697us/step - loss: 12.6076 - val_loss: 12.3650\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 44s 739us/step - loss: 12.3671 - val_loss: 12.1008\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 43s 710us/step - loss: 12.1575 - val_loss: 11.9138\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 42s 706us/step - loss: 11.9888 - val_loss: 11.7684\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 42s 703us/step - loss: 11.8449 - val_loss: 11.6270\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 44s 740us/step - loss: 11.7163 - val_loss: 11.5222\n",
      "Epoch 13/100\n",
      "33536/60000 [===============>..............] - ETA: 18s - loss: 11.6399"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8d39e3e65261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 validation_data=(x_test, x_test))\n\u001b[0m",
      "\u001b[0;32m~/p35/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/p35/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p35/lib/python3.5/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/p35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=128,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(input_dim,))\n",
    "encoder_layer1 = autoencoder.layers[0]\n",
    "encoder_layer2 = autoencoder.layers[1]\n",
    "encoder_layer3 = autoencoder.layers[2]\n",
    "encoder = Model(input_img, encoder_layer3(encoder_layer2(encoder_layer1(input_img))))\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model\n",
    "\n",
    "Un discriminant unique pour chaque problème de chaque dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p35_jupyter",
   "language": "python",
   "name": "p35_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
